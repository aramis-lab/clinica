<!-- markdownlint-disable MD046 -->
# `deeplearning-prepare-data` - Prepare input data for deep learning with PyTorch

This pipeline prepares images generated by Clinica to be used with the PyTorch
deep learning library [[Paszke et al.,
2019]](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library).
Four types of tensors are proposed: 3D images, 3D patches, 2D slices or 3D 
region of interest (ROI).

Outputs from the [`t1-linear` pipeline](../T1_Linear), [`t1-extensive`
pipeline](https://clinicadl.readthedocs.io/en/latest/Preprocessing/T1_Extensive/) and 
[`pet-linear` pipeline](../PET_Linear) can be processed (for the moment, 
`t1-extensive` is available from [ClinicaDL](https://clinicadl.readthedocs.io)).
These pipelines are designed as a prerequisite for
the deep learning classification algorithms presented in
[[Wen et al., 2020](https://doi.org/10.1016/j.media.2020.101694)] and showcased
in the [AD-DL framework](https://github.com/aramis-lab/AD-DL).

## Prerequisites

<!-- Depending on the type of feature or the type of modality you want to use,
you will need to execute either the [`t1-linear` pipeline](../T1_Linear) , the
[`t1-volume` pipeline](../T1_Volume) and/or the [`pet-volume`
pipeline](../PET_Volume)  prior to running this pipeline. -->

You need to have performed the [`t1-linear` pipeline](../T1_Linear) or
[`t1-extensive`
pipeline](https://clinicadl.readthedocs.io/en/latest/Preprocessing/T1_Extensive/)
on your T1-weighted MRI or [`pet-linear` pipeline](../PET_Linear) on
your PET images.
There exists an option to convert custom nifty images into tensors.
This option is chosen by adding the parameter `custom` to the
command line instruction and a known suffix, as explained below.

## Dependencies

If you installed the core of Clinica, this pipeline needs no further dependencies.

## Running the pipeline

The pipeline can be run with the following command line:

```Text
clinica run deeplearning-prepare-data <caps_directory> <modality> <tensor_format>
```

where:

- `caps_directory` is the folder containing the results of the
[`t1-linear` pipeline](../T1_Linear) and the output of the present command,
both in a [CAPS hierarchy](../../CAPS/Introduction).
- `modality` is the name of the preprocessing done in the original images.
It can be `t1-linear`, `t1-extensive` or `pet-linear`.
You can chose `custom` if you want to get a tensor from a custom filename.
- `tensor_format` is the format of the extracted tensors.
You can choose between `image` to convert to PyTorch tensor the whole 3D image,
`patch` to extract 3D patches, `slice` to extract 2D slices from the image and
`roi` to extract 3D region of interest depending on the ROI mask.

By default the features are extracted from the cropped image (see the
documentation of the [`t1-linear` pipeline](../T1_Linear)).
You can deactivate this behaviour with the `--use_uncropped_image` flag.
The same behaviour is applied to [`pet-linear` pipeline](../PET_Linear).

Pipeline options if you use `patch` extraction:

- `--patch_size`: patch size. Default value: `50`.
- `--stride_size`:  stride size. Default value: `50`.

Pipeline options if you use `slice` extraction:

- `--slice_direction`: slice direction.
You can choose between `0` (sagittal plane), `1`(coronal plane) or
`2` (axial plane). Default value: `0`.
- `--slice_mode`: slice mode.
You can choose between `rgb` (will save the slice in three identical channels) or
`single` (will save the slice in a single channel). Default value: `rgb`.

Pipeline options if you use `roi` extraction:

- `--roi_list`: list of ROI to be extracted. By default the masks of ROI should be
save in `<caps_directory/masks>`.
- `--roi_uncrop_output`: disable cropping option so the output 
tensors have the same size than the whole image instead of the ROI size.
- `--custom_template` (optionnal): if the modality is set to custom, it will chose the 
corresponding mask which matches with the template.
- `--custom_mask_pattern` (optionnal): to select a mask with a specific pattern 
in the file name.

Pipeline options if you use `pet-linear` modality:

- `--acq_label`: the label given to the PET acquisition, specifying the tracer
used (`acq-<acq_label>`). It can be for instance 'fdg' for
<sup>18</sup>F-fluorodeoxyglucose or 'av45' for <sup>18</sup>F-florbetapir;
- `--suvr_reference_region`: the reference region used to perform intensity
normalization (i.e. dividing each voxel of the image by the average uptake in
this region) resulting in a standardized uptake value ratio (SUVR) map. It can
be `cerebellumPons` or `cerebellumPons2` (used for amyloid tracers) and `pons` 
or `pons2` (used for FDG). See [PET introduction](./PET_Introduction.md) for 
more details about masks versions.

!!! note
    Same values used for the [`pet-linear` pipeline](../PET_Linear) are needed for
    both options.

Pipeline options if you use `custom` modality:

- `--custom_suffix`: suffix of the filename that should be converted to the tensor format.
The output will be saved into a folder named `custom` but
the processed files will keep their original name.
For instance, you can convert the images from the segmentation of
the grey matter registered on the Ixi549Space.
These images are obtained by running `t1-volume` pipeline (and SPM underhood).
The suffix for these images is `graymatter_space-Ixi549Space_modulated-off_probability.nii.gz`.

!!! note "Regarding the default values"
    When using patch or slice extraction, default values were set according to
    [[Wen et al., 2020](https://doi.org/10.1016/j.media.2020.101694)].

!!! note
    The arguments common to all Clinica pipelines are described in [Interacting
    with clinica](../InteractingWithClinica).

!!! tip
    Do not hesitate to type `clinica run deeplearning-prepare-data --help` to see
    the full list of parameters.

## Outputs

In the following subsections, files with the `.pt` extension denote tensors in
PyTorch format.

The full list of output files can be found in the [ClinicA Processed Structure (CAPS)
Specification](../../CAPS/Specifications/#deeplearning-prepare-data-prepare-input-data-for-deep-learning-with-pytorch).

### Image-based outputs

Results are stored in the following folder of the
[CAPS hierarchy](../../CAPS/Specifications):
`subjects/<participant_id>/<session_id>/deeplearning_prepare_data/image_based/t1_linear`.

For the `t1-linear` modality, the main output files are:

- `<source_file>_space-MNI152NLin2009cSym[_desc-Crop]_res-1x1x1_T1w.pt`: tensor
  version of the 3D T1w image registered to the [`MNI152NLin2009cSym`
  template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html)
  and optionally cropped.

Corresponding folder and file names are obtained for the files processed with the
`t1-extensive` and `pet-linear` modality.

For the case of files processed with the `custom` modality, files are stored in
the following folder:
`subjects/<participant_id>/<session_id>/deeplearning_prepare_data/image/custom`.

### Patch-based outputs

Results are stored in the following folder of the
[CAPS hierarchy](../../CAPS/Specifications):
`subjects/<participant_id>/<session_id>/deeplearning_prepare_data/patch_based/t1_linear`.

The main output files are:

- `<source_file>_space-MNI152NLin2009cSym[_desc-Crop]_res-1x1x1_patchsize-<N>_stride-<M>_patch-<i>_T1w.pt`:
tensor version of the `<i>`-th 3D isotropic patch of size `<N>` with a stride of `<M>`.
Each patch is extracted from the T1w image registered to the [`MNI152NLin2009cSym`
template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html)
and optionally cropped.

### Slice-based outputs

Results are stored in the following folder of the
[CAPS hierarchy](../../CAPS/Specifications):
`subjects/<participant_id>/<session_id>/deeplearning_prepare_data/slice_based/t1_linear`.

The main output files are:

- `<source_file>_space-MNI152NLin2009cSym[_desc-Crop]_res-1x1x1_axis-{sag|cor|axi}_channel-{single|rgb}_T1w.pt`:
tensor version of the `<i>`-th 2D slice in `sag`ittal, `cor`onal or `axi`al
plane using three identical channels (`rgb`) or one channel (`single`).
Each slice is extracted from the T1w image registered to the [`MNI152NLin2009cSym`
template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html)
and optionally cropped.

### ROI based outputs

Results are stored in the following folder of the
[CAPS hierarchy](../../CAPS/Specifications):
`subjects/<participant_id>/<session_id>/deeplearning_prepare_data/roi_based/t1_linear`.

The main output files are:

`<source_file>_space-MNI152NLin2009cSym[_desc-{CropRoi|CropImage}]_res-1x1x1_roi-<name_of_roi>_T1w.pt`:
tensor version of the selected 3D region of interest. 
Each slice is extracted from the T1w image registered to the [`MNI152NLin2009cSym`
template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html)
and optionally cropped.

## Going further

- You can now perform classification based on deep learning using the
[AD-DL framework](https://github.com/aramis-lab/AD-DL) presented in
[[Wen et al., 2020](https://doi.org/10.1016/j.media.2020.101694)].

## Describing this pipeline in your paper

!!! cite "Example of paragraph"
    These results have been obtained using the `deeplearning-prepare-data`
    pipeline of Clinica [[Routier et al](https://hal.inria.fr/hal-02308126/);
    [Wen et al., 2020](https://doi.org/10.1016/j.media.2020.101694)].
    More precisely,

    - 3D images

    - 3D patches with patch size of `<patch_size>` and stride size of
      `<stride_size>`

    - 2D slices in {sagittal | coronal | axial} plane and saved in {three
      identical channels | a single channel}

    - 3D regions of interest depending on a mask given in input

    were extracted and converted to PyTorch tensors [[Paszke et al.,
    2019](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library)].

!!! tip
    Easily access the papers cited on this page on
    [Zotero](https://www.zotero.org/groups/2240070/clinica_aramislab/collections/8B2R2826).
